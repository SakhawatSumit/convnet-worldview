{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lasagne Conv Net on Media Eval 2015 Placing Task - Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import time\n",
    "\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import own modules\n",
    "import data_utils, visualize\n",
    "import lasagne_model_predict_country as cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "cnn_architecture = \"complex_cnn\"\n",
    "num_filters = 32\n",
    "filter_width = 3 # can be integer or tuple\n",
    "pool_width = 2 \n",
    "stride_width = 1 # can be integer or tuple\n",
    "padding = 'full'  # can be integer or tuple or 'full', 'same', 'valid'\n",
    "hidden_size = 256 # size of hidden layer of neurons\n",
    "dropout_p = 0.0\n",
    "# lr_decay = 0.995\n",
    "reg_strength = 0\n",
    "# grad_clip = 10\n",
    "\n",
    "# Optimization hyperparams\n",
    "# LEARNING_RATE = 1e-2\n",
    "LEARNING_RATE = 0.045\n",
    "\n",
    "# USE_OPTIMIZER = \"nesterov_momentum\"\n",
    "USE_OPTIMIZER = \"adam\"\n",
    "# (1) Nesterov Momentum\n",
    "MOMENTUM = 0.9\n",
    "# (2) Adam\n",
    "beta1=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-08\n",
    "# Optimizer config\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "# Training parameters\n",
    "batchsize = 1000\n",
    "num_epochs = 10\n",
    "record_per_iter = True  # save train and val loss/accuracy after each batch runthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data Set:', 'subset_48by32_5')\n",
      "('Num classes:', 5)\n",
      "Batches: [0, 1, 2]\n",
      "Preparing Data Set....\n",
      "X: (90000, 32, 48, 3)\n",
      "Y: (90000,)\n",
      "Y sample  [0 1 2 1 0 3 2 2 1 2]\n",
      "('X_train', (72000, 3, 48, 32))\n",
      "('y_train', (72000,))\n",
      "('X_val', (9000, 3, 48, 32))\n",
      "('y_val', (9000,))\n",
      "('X_test', (9000, 3, 48, 32))\n",
      "('y_test', (9000,))\n"
     ]
    }
   ],
   "source": [
    "# Load Data Set\n",
    "\n",
    "# DATA_BATCH = '000_small_'\n",
    "# DATA_SIZE = '48by32'\n",
    "# DATA_SET = DATA_BATCH + DATA_SIZE\n",
    "# NUM_CLASSES = 5\n",
    "\n",
    "DATA_SET = 'subset_48by32_5'\n",
    "NUM_CLASSES = 5\n",
    "NUM_BATCHES = 6\n",
    "USE_BATCH = [0, 1, 2]\n",
    "\n",
    "print ('Data Set:', DATA_SET)\n",
    "print ('Num classes:', NUM_CLASSES)\n",
    "print ('Batches: {}'.format(USE_BATCH))\n",
    "print ('Preparing Data Set....')\n",
    "\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "for batch_num in USE_BATCH:\n",
    "    X_input_filename = 'data_maps/' + DATA_SET + '/x_input_' + str(batch_num) + '.npy'\n",
    "    Y_output_filename = 'data_maps/' + DATA_SET + '/y_labels_' + str(batch_num) + '.npy'\n",
    "\n",
    "    X_batch = data_utils.load_npy_file(X_input_filename)\n",
    "    Y_batch = data_utils.load_npy_file(Y_output_filename)\n",
    "    \n",
    "    if X is None:\n",
    "        X = X_batch\n",
    "    else:\n",
    "        X = np.vstack((X, X_batch))\n",
    "        \n",
    "    if Y is None:\n",
    "        Y = Y_batch\n",
    "    else:\n",
    "        Y = np.hstack((Y, Y_batch))  # use hstack because 1-d array is represented as a row vector internally\n",
    "    \n",
    "    \n",
    "print 'X: {}'.format(X.shape)\n",
    "print 'Y: {}'.format(Y.shape)\n",
    "print 'Y sample ', Y[:10]\n",
    "\n",
    "num_samples, H, W, C = X.shape\n",
    "\n",
    "# swap C and H axes --> expected input\n",
    "X = np.swapaxes(X, 1, 3)  # (num_samples, C, W, H)\n",
    "X -= np.mean(X, axis = 0)  # Data Preprocessing: mean subtraction\n",
    "X /= np.std(X, axis = 0)  # Normalization\n",
    "\n",
    "#Splitting into train, val, test sets\n",
    "\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = int(num_samples * 0.1)\n",
    "num_test = num_samples - num_train - num_val\n",
    "\n",
    "# print 'num_train: %d, num_val: %d, num_test: %d' % (num_train, num_val, num_test)\n",
    "\n",
    "X_train = X[:num_train]\n",
    "X_val = X[num_train:num_train+num_val]\n",
    "X_test = X[num_train+num_val:]\n",
    "\n",
    "y_train = Y[:num_train]\n",
    "y_val = Y[num_train:num_train+num_val]\n",
    "y_test = Y[num_train+num_val:]\n",
    "\n",
    "print ('X_train', X_train.shape)\n",
    "print ('y_train', y_train.shape)\n",
    "print ('X_val', X_val.shape)\n",
    "print ('y_val', y_val.shape)\n",
    "print ('X_test', X_test.shape)\n",
    "print ('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network...\n",
      "Building a complex CNN...\n",
      "Compiling functions...\n",
      "Using Update:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '843' (I am process '1207')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '843' (I am process '1207')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nesterov_momentum\n",
      "Compiling Finished!\n"
     ]
    }
   ],
   "source": [
    "# Create model and compile train and val functions\n",
    "train_fn, val_fn = cnn_model.main_create_model(C, W, H, NUM_CLASSES, cnn_architecture=cnn_architecture, num_filters=num_filters, filter_width=filter_width, pool_width=pool_width, stride=stride_width, pad=padding, hidden_size=hidden_size, dropout=dropout_p, use_optimizer=USE_OPTIMIZER, learning_rate=LEARNING_RATE, momentum=MOMENTUM, beta1=beta1, beta2=beta2, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: 10 epochs of batch size 500 with num training samples 72000\n",
      "Using optimizer: nesterov_momentum\n",
      "Validation Size: 9000\n",
      "Starting training...\n",
      "Ep 0 \titer 0  \tloss 1.64374, train acc 19.60, val acc 23.00\n",
      "Ep 0 \titer 1  \tloss 1.61998, train acc 20.60, val acc 26.60\n",
      "Ep 0 \titer 2  \tloss 1.60688, train acc 22.20, val acc 24.80\n",
      "Ep 0 \titer 3  \tloss 1.61625, train acc 22.80, val acc 23.60\n",
      "Ep 0 \titer 4  \tloss 1.60822, train acc 20.60, val acc 26.00\n",
      "Ep 0 \titer 5  \tloss 1.60186, train acc 21.20, val acc 26.20\n",
      "Ep 0 \titer 6  \tloss 1.62144, train acc 24.20, val acc 26.80\n",
      "Ep 0 \titer 7  \tloss 1.59839, train acc 24.20, val acc 24.20\n",
      "Ep 0 \titer 8  \tloss 1.60747, train acc 23.20, val acc 25.60\n",
      "Ep 0 \titer 9  \tloss 1.59754, train acc 24.40, val acc 25.20\n",
      "Ep 0 \titer 10  \tloss 1.58525, train acc 27.80, val acc 29.40\n",
      "Ep 0 \titer 11  \tloss 1.60335, train acc 23.40, val acc 23.60\n",
      "Ep 0 \titer 12  \tloss 1.59306, train acc 24.60, val acc 24.80\n",
      "Ep 0 \titer 13  \tloss 1.60430, train acc 23.60, val acc 23.80\n",
      "Ep 0 \titer 14  \tloss 1.59937, train acc 24.00, val acc 23.80\n",
      "Ep 0 \titer 15  \tloss 1.59595, train acc 23.80, val acc 23.00\n",
      "Ep 0 \titer 16  \tloss 1.59539, train acc 25.00, val acc 26.20\n",
      "Ep 0 \titer 17  \tloss 1.59120, train acc 26.20, val acc 28.00\n",
      "Ep 0 \titer 18  \tloss 1.61381, train acc 21.20, val acc 25.60\n",
      "Ep 0 \titer 19  \tloss 1.58750, train acc 22.60, val acc 24.00\n",
      "Ep 0 \titer 20  \tloss 1.61156, train acc 19.20, val acc 23.20\n",
      "Ep 0 \titer 21  \tloss 1.60753, train acc 23.00, val acc 26.00\n",
      "Ep 0 \titer 22  \tloss 1.58706, train acc 27.60, val acc 27.20\n",
      "Ep 0 \titer 23  \tloss 1.59497, train acc 24.80, val acc 24.80\n",
      "Ep 0 \titer 24  \tloss 1.59808, train acc 24.60, val acc 25.20\n",
      "Ep 0 \titer 25  \tloss 1.60649, train acc 21.40, val acc 22.20\n",
      "Ep 0 \titer 26  \tloss 1.60754, train acc 24.60, val acc 26.60\n",
      "Ep 0 \titer 27  \tloss 1.59621, train acc 23.60, val acc 23.40\n",
      "Ep 0 \titer 28  \tloss 1.59945, train acc 23.80, val acc 26.80\n",
      "Ep 0 \titer 29  \tloss 1.60403, train acc 21.20, val acc 24.00\n",
      "Ep 0 \titer 30  \tloss 1.60619, train acc 22.40, val acc 23.60\n",
      "Ep 0 \titer 31  \tloss 1.59134, train acc 24.60, val acc 25.20\n",
      "Ep 0 \titer 32  \tloss 1.59723, train acc 26.00, val acc 27.20\n",
      "Ep 0 \titer 33  \tloss 1.59206, train acc 26.80, val acc 27.00\n",
      "Ep 0 \titer 34  \tloss 1.59553, train acc 26.20, val acc 26.20\n",
      "Ep 0 \titer 35  \tloss 1.60361, train acc 23.80, val acc 23.60\n",
      "Ep 0 \titer 36  \tloss 1.59530, train acc 23.40, val acc 25.20\n",
      "Ep 0 \titer 37  \tloss 1.60228, train acc 25.40, val acc 27.00\n",
      "Ep 0 \titer 38  \tloss 1.60286, train acc 23.60, val acc 25.60\n",
      "Ep 0 \titer 39  \tloss 1.61197, train acc 21.60, val acc 22.20\n",
      "Ep 0 \titer 40  \tloss 1.58797, train acc 25.40, val acc 26.20\n",
      "Ep 0 \titer 41  \tloss 1.60326, train acc 23.40, val acc 27.00\n",
      "Ep 0 \titer 42  \tloss 1.60741, train acc 21.80, val acc 22.60\n",
      "Ep 0 \titer 43  \tloss 1.60592, train acc 24.20, val acc 24.60\n",
      "Ep 0 \titer 44  \tloss 1.60211, train acc 24.80, val acc 25.00\n",
      "Ep 0 \titer 45  \tloss 1.59358, train acc 26.20, val acc 26.60\n",
      "Ep 0 \titer 46  \tloss 1.61600, train acc 21.80, val acc 22.80\n",
      "Ep 0 \titer 47  \tloss 1.59155, train acc 26.60, val acc 27.20\n",
      "Ep 0 \titer 48  \tloss 1.59176, train acc 25.60, val acc 26.60\n",
      "Ep 0 \titer 49  \tloss 1.59588, train acc 26.60, val acc 27.20\n",
      "Ep 0 \titer 50  \tloss 1.60949, train acc 23.40, val acc 23.20\n",
      "Ep 0 \titer 51  \tloss 1.60990, train acc 22.60, val acc 23.40\n",
      "Ep 0 \titer 52  \tloss 1.59479, train acc 26.60, val acc 26.20\n",
      "Ep 0 \titer 53  \tloss 1.59240, train acc 25.00, val acc 24.80\n",
      "Ep 0 \titer 54  \tloss 1.58626, train acc 26.00, val acc 26.20\n",
      "Ep 0 \titer 55  \tloss 1.59304, train acc 25.00, val acc 24.80\n",
      "Ep 0 \titer 56  \tloss 1.59552, train acc 23.80, val acc 25.40\n",
      "Ep 0 \titer 57  \tloss 1.60169, train acc 22.20, val acc 23.00\n",
      "Ep 0 \titer 58  \tloss 1.59167, train acc 28.60, val acc 29.60\n",
      "Ep 0 \titer 59  \tloss 1.59790, train acc 25.00, val acc 26.00\n",
      "Ep 0 \titer 60  \tloss 1.61974, train acc 18.80, val acc 20.60\n",
      "Ep 0 \titer 61  \tloss 1.59002, train acc 26.60, val acc 28.00\n",
      "Ep 0 \titer 62  \tloss 1.58324, train acc 28.60, val acc 30.40\n",
      "Ep 0 \titer 63  \tloss 1.61258, train acc 20.20, val acc 21.60\n",
      "Ep 0 \titer 64  \tloss 1.59325, train acc 24.40, val acc 24.80\n",
      "Ep 0 \titer 65  \tloss 1.59520, train acc 22.80, val acc 23.00\n",
      "Ep 0 \titer 66  \tloss 1.59600, train acc 24.80, val acc 26.60\n",
      "Ep 0 \titer 67  \tloss 1.59274, train acc 26.40, val acc 26.20\n",
      "Ep 0 \titer 68  \tloss 1.58962, train acc 24.80, val acc 25.40\n",
      "Ep 0 \titer 69  \tloss 1.59960, train acc 23.00, val acc 22.80\n",
      "Ep 0 \titer 70  \tloss 1.59881, train acc 24.40, val acc 24.00\n",
      "Ep 0 \titer 71  \tloss 1.59364, train acc 26.40, val acc 27.20\n",
      "Ep 0 \titer 72  \tloss 1.58119, train acc 27.40, val acc 27.20\n",
      "Ep 0 \titer 73  \tloss 1.58624, train acc 29.00, val acc 29.20\n",
      "Ep 0 \titer 74  \tloss 1.57446, train acc 27.80, val acc 27.60\n",
      "Ep 0 \titer 75  \tloss 1.59587, train acc 25.60, val acc 25.60\n",
      "Ep 0 \titer 76  \tloss 1.59763, train acc 24.60, val acc 25.00\n",
      "Ep 0 \titer 77  \tloss 1.59241, train acc 24.60, val acc 25.20"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "train_err_list, train_acc_list, val_err_list, val_acc_list, epochs_train_err_list, epochs_train_acc_list, epochs_val_err_list, epochs_val_acc_list = cnn_model.train(num_epochs, batchsize, num_train, num_val, USE_OPTIMIZER, train_fn, val_fn, X_train, y_train, record_per_iter=record_per_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "print (\"Test Size: {}\".format(num_test) )\n",
    "print('Testing...')\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in data_utils.iterate_minibatches(X_test, y_test, batchsize, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "num_train = X_train.shape[0]\n",
    "if record_per_iter:\n",
    "    xlabel = \"iterations\"\n",
    "else:\n",
    "    xlabel = \"epochs\"\n",
    "# Printing training losses and training + validation accuracies\n",
    "data_set_name = DATA_SET + 'batch'\n",
    "for batch_num in USE_BATCH:\n",
    "    data_set_name += \"_\"\n",
    "    data_set_name += str(batch_num)\n",
    "visualize.plot_loss_acc(data_set_name, train_err_list, train_acc_list, val_acc_list, LEARNING_RATE, reg_strength, num_epochs, num_train, xlabel=xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "num_train = X_train.shape[0]\n",
    "xlabel = \"epochs\"\n",
    "# Printing training losses and training + validation accuracies\n",
    "data_set_name = DATA_SET + 'batch_' + str(USE_BATCH) + '_on_epochs' + '_' + USE_OPTIMIZER\n",
    "visualize.plot_loss_acc(data_set_name, epochs_train_err_list, epochs_train_acc_list, epochs_val_acc_list, LEARNING_RATE, reg_strength, num_epochs, num_train, xlabel=xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

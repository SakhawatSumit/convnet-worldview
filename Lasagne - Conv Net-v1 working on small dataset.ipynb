{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lasagne Conv Net on Media Eval 2015 Placing Task - Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import own modules\n",
    "import data_utils\n",
    "import lasagne_model_predict_country as cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "num_filters = 32\n",
    "filter_width = 5\n",
    "pool_width = 2\n",
    "hidden_size = 256 # size of hidden layer of neurons\n",
    "dropout_p = 0.0\n",
    "# lr_decay = 0.995\n",
    "# reg_strength = 2e-2\n",
    "# grad_clip = 10\n",
    "\n",
    "# Optimization hyperparams\n",
    "LEARNING_RATE = 1e-2\n",
    "MOMENTUM = 0.9\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "# Training parameters\n",
    "batchsize = 32\n",
    "num_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set: 000_small_48by32\n",
      "Num classes: 5\n",
      "Preparing Data Set....\n",
      "X_train (1504, 3, 48, 32)\n",
      "y_train (1504,)\n",
      "X_val (188, 3, 48, 32)\n",
      "X_test (188, 3, 48, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load Data Set\n",
    "\n",
    "DATA_BATCH = '000_small_'\n",
    "DATA_SIZE = '48by32'\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "\n",
    "DATA_SET = DATA_BATCH + DATA_SIZE\n",
    "print 'Data Set:', DATA_SET\n",
    "print 'Num classes:', NUM_CLASSES\n",
    "print 'Preparing Data Set....'\n",
    "\n",
    "X_input_filename = 'data_maps/' + DATA_SET + '/x_input.npy'\n",
    "Y_output_filename = 'data_maps/' + DATA_SET + '/y_labels.npy'\n",
    "\n",
    "X = data_utils.load_npy_file(X_input_filename)\n",
    "Y = data_utils.load_npy_file(Y_output_filename)\n",
    "# print 'X: {}'.format(X.shape)\n",
    "# print 'Y: {}'.format(Y.shape)\n",
    "# print 'Y sample ', Y[:10]\n",
    "\n",
    "num_samples, H, W, C = X.shape\n",
    "\n",
    "# swap C and H axes --> expected input\n",
    "X = np.swapaxes(X, 1, 3)  # (num_samples, C, W, H)\n",
    "X -= np.mean(X, axis = 0)  # Data Preprocessing: mean subtraction\n",
    "\n",
    "#Splitting into train, val, test sets\n",
    "\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = int(num_samples * 0.1)\n",
    "num_test = num_samples - num_train - num_val\n",
    "\n",
    "# print 'num_train: %d, num_val: %d, num_test: %d' % (num_train, num_val, num_test)\n",
    "\n",
    "X_train = X[:num_train]\n",
    "X_val = X[num_train:num_train+num_val]\n",
    "X_test = X[num_train+num_val:]\n",
    "\n",
    "y_train = Y[:num_train]\n",
    "y_val = Y[num_train:num_train+num_val]\n",
    "y_test = Y[num_train+num_val:]\n",
    "\n",
    "print 'X_train', X_train.shape\n",
    "print 'y_train', y_train.shape\n",
    "print 'X_val', X_val.shape\n",
    "print 'X_test', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network...\n",
      "Compiling functions...\n",
      "Compiling Finished!\n",
      "Starting training...\n",
      "Epoch 1 of 8 took 42.399s\n",
      "  training loss:\t\t37445.603316\n",
      "  training accuracy:\t\t10.22 %\n",
      "  validation loss:\t\t1.602254\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 2 of 8 took 38.128s\n",
      "  training loss:\t\t1.603228\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.598242\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 3 of 8 took 36.933s\n",
      "  training loss:\t\t1.602582\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596488\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 4 of 8 took 37.119s\n",
      "  training loss:\t\t1.602509\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596682\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 5 of 8 took 36.458s\n",
      "  training loss:\t\t1.603105\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596970\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 6 of 8 took 36.296s\n",
      "  training loss:\t\t1.602438\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596114\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 7 of 8 took 36.385s\n",
      "  training loss:\t\t1.602693\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596490\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Epoch 8 of 8 took 36.347s\n",
      "  training loss:\t\t1.602340\n",
      "  training accuracy:\t\t10.59 %\n",
      "  validation loss:\t\t1.596285\n",
      "  validation accuracy:\t\t21.88 %\n",
      "Training finished!\n",
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t1.604985\n",
      "  test accuracy:\t\t20.62 %\n"
     ]
    }
   ],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = Tensor.tensor4('inputs')\n",
    "target_var = Tensor.ivector('targets')\n",
    "\n",
    "print('Building network...')\n",
    "\n",
    "# Create neural network model\n",
    "l_in, l_out = cnn_model.build_cnn(C, W, H, NUM_CLASSES, num_filters=num_filters, filter_width=filter_width, pool_width=pool_width, hidden_size=hidden_size, dropout=dropout_p, inputVar = input_var)\n",
    "\n",
    "print('Compiling functions...')\n",
    "\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(l_out)\n",
    "loss = Tensor.nnet.categorical_crossentropy(prediction, target_var)\n",
    "# loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "acc = Tensor.mean(Tensor.eq(Tensor.argmax(prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Return predictions in a function\n",
    "pred_fn = theano.function([l_in.input_var], prediction)\n",
    "\n",
    " # Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(l_out, deterministic=True)\n",
    "# test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = Tensor.nnet.categorical_crossentropy(test_prediction, target_var)\n",
    "\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = Tensor.mean(Tensor.eq(Tensor.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    " # Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], [loss, acc], updates=updates)\n",
    "# train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "print('Compiling Finished!')\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1) In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in data_utils.iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        err, acc = train_fn(inputs, targets)\n",
    "        prediction = pred_fn(inputs)\n",
    "        train_err += err\n",
    "        train_acc += acc\n",
    "        train_batches += 1\n",
    "            \n",
    "    # 2) And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in data_utils.iterate_minibatches(X_val, y_val, batchsize, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(train_acc))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "print('Training finished!')\n",
    "\n",
    "    \n",
    "# After training, we compute and print the test error:\n",
    "print('Testing...')\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in data_utils.iterate_minibatches(X_test, y_test, batchsize, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "num_train = train_data[0].shape[0]\n",
    "visualize.plot_loss_acc('subset_5_train', train_losses, train_corrected_accs, val_corrected_accs, learning_rate, reg_strength, num_epochs, num_train, xlabel='iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
